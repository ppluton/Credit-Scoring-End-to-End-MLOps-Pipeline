{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453d8a04",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47eb3777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports r√©ussis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/13 16:12:36 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/02/13 16:12:36 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/02/13 16:12:36 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/13 16:12:36 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/02/13 16:12:36 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/13 16:12:36 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exp√©rience trouv√©e: home_credit_default_risk (ID: 1)\n",
      "‚úÖ Tracking URI: sqlite:///mlflow.db\n",
      "‚úÖ Exp√©rience active: home_credit_default_risk\n",
      "‚úÖ Donn√©es charg√©es: (307511, 421)\n",
      "‚úÖ Train: (246008, 419) - {0: 0.9192709180189262, 1: 0.08072908198107379}\n",
      "‚úÖ Val:   (61503, 419) - {0: 0.9192722306228964, 1: 0.08072776937710356}\n",
      "üîí Val isol√© jusqu'√† l'√©valuation finale\n",
      "‚úÖ X_train shape: (246008, 419)\n",
      "‚úÖ y_train distribution: {0: 226148, 1: 19860}\n",
      "\n",
      "============================================================\n",
      "üéØ CONFIGURATION TERMIN√âE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.base import clone\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")\n",
    "\n",
    "# 1.1.1 Param√®tres globaux\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "COST_FN = 10  # Co√ªt d'un Faux N√©gatif = 10x Faux Positif\n",
    "\n",
    "# 1.2 Configuration MLFlow - CORRIG√âE\n",
    "EXPERIMENT_NAME = \"home_credit_default_risk\"\n",
    "\n",
    "# IMPORTANT : V√©rifier si l'exp√©rience existe, sinon la cr√©er\n",
    "try:\n",
    "    # Essayer de r√©cup√©rer l'exp√©rience existante\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "    if experiment is None:\n",
    "        # L'exp√©rience n'existe pas, on la cr√©e\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"‚úÖ Exp√©rience cr√©√©e: {EXPERIMENT_NAME} (ID: {experiment_id})\")\n",
    "    else:\n",
    "        # L'exp√©rience existe d√©j√†\n",
    "        experiment_id = experiment.experiment_id\n",
    "        print(f\"‚úÖ Exp√©rience trouv√©e: {EXPERIMENT_NAME} (ID: {experiment_id})\")\n",
    "\n",
    "    # D√©finir l'exp√©rience active\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur MLFlow: {e}\")\n",
    "    print(\"üí° Solution : Cr√©ation manuelle de l'exp√©rience...\")\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    print(f\"‚úÖ Exp√©rience cr√©√©e: {EXPERIMENT_NAME} (ID: {experiment_id})\")\n",
    "\n",
    "print(f\"‚úÖ Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"‚úÖ Exp√©rience active: {mlflow.get_experiment_by_name(EXPERIMENT_NAME).name}\")\n",
    "\n",
    "# 1.3 Charger les donn√©es preprocessed\n",
    "DATA_PATH = Path(\"../data\")\n",
    "train_full = pd.read_csv(DATA_PATH / \"train_preprocessed.csv\")\n",
    "\n",
    "print(f\"‚úÖ Donn√©es charg√©es: {train_full.shape}\")\n",
    "\n",
    "# S√©parer X, y\n",
    "X_full = train_full.drop([\"TARGET\", \"SK_ID_CURR\"], axis=1)\n",
    "y_full = train_full[\"TARGET\"]\n",
    "\n",
    "# üîÄ SPLIT 80/20 pour cr√©er un set de validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, stratify=y_full, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train: {X_train.shape} - {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"‚úÖ Val:   {X_val.shape} - {y_val.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"üîí Val isol√© jusqu'√† l'√©valuation finale\")\n",
    "\n",
    "\n",
    "print(f\"‚úÖ X_train shape: {X_train.shape}\")\n",
    "print(f\"‚úÖ y_train distribution: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ CONFIGURATION TERMIN√âE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c729986",
   "metadata": {},
   "source": [
    "## # ANALYSE DES MONTANTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cde73943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä MONTANTS DE CR√âDIT:\n",
      "   Moyenne : 599,026\n",
      "   M√©diane : 513,531\n",
      "   Min     : 45,000\n",
      "   Max     : 4,050,000\n",
      "\n",
      "üí∞ Total des cr√©dits : 184,207,084,196\n",
      "üë• Nombre de clients : 307,511\n",
      "üìâ Taux de d√©faut : 8.07%\n"
     ]
    }
   ],
   "source": [
    "train_agregatted = pd.read_csv(DATA_PATH / \"train_aggregated.csv\")\n",
    "\n",
    "# Montant moyen du cr√©dit\n",
    "amt_credit_mean = train_agregatted[\"AMT_CREDIT\"].mean()\n",
    "amt_credit_median = train_agregatted[\"AMT_CREDIT\"].median()\n",
    "\n",
    "print(f\"üìä MONTANTS DE CR√âDIT:\")\n",
    "print(f\"   Moyenne : {amt_credit_mean:,.0f}\")\n",
    "print(f\"   M√©diane : {amt_credit_median:,.0f}\")\n",
    "print(f\"   Min     : {train_agregatted['AMT_CREDIT'].min():,.0f}\")\n",
    "print(f\"   Max     : {train_agregatted['AMT_CREDIT'].max():,.0f}\")\n",
    "\n",
    "# Montant total des cr√©dits\n",
    "total_credits = train_agregatted[\"AMT_CREDIT\"].sum()\n",
    "print(f\"\\nüí∞ Total des cr√©dits : {total_credits:,.0f}\")\n",
    "\n",
    "# Volume\n",
    "n_clients = len(train_agregatted)\n",
    "print(f\"üë• Nombre de clients : {n_clients:,}\")\n",
    "\n",
    "# Taux de d√©faut dans les donn√©es\n",
    "default_rate = train_agregatted[\"TARGET\"].mean()\n",
    "print(f\"üìâ Taux de d√©faut : {default_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546a0d5",
   "metadata": {},
   "source": [
    "## 3. Score M√©tier Personnalis√©\n",
    "\n",
    "Le d√©s√©quilibre du co√ªt m√©tier :\n",
    "\n",
    "- Faux N√©gatif (FN) : Mauvais client pr√©dit bon ‚Üí Cr√©dit accord√© ‚Üí PERTE\n",
    "- Faux Positif (FP) : Bon client pr√©dit mauvais ‚Üí Cr√©dit refus√© ‚Üí Manque √† gagner\n",
    "\n",
    "Hypoth√®se : Co√ªt FN = 10 √ó Co√ªt FP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed05542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test du Score M√©tier:\n",
      "============================================================\n",
      "Mod√®le parfait : Co√ªt = 0.0000\n",
      "1 Faux N√©gatif : Co√ªt = 2.5000 (= 10/4 = 2.50)\n",
      "1 Faux Positif : Co√ªt = 0.2500 (= 1/4 = 0.25)\n",
      "\n",
      "üí° Un FN co√ªte 10x plus qu'un FP\n"
     ]
    }
   ],
   "source": [
    "def business_cost_score(y_true, y_pred, cost_fn=10, cost_fp=1):\n",
    "    \"\"\"\n",
    "    Calcule le co√ªt m√©tier total.\n",
    "\n",
    "    Plus le score est BAS, meilleur est le mod√®le.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Vraies valeurs (0 ou 1)\n",
    "    y_pred : array-like\n",
    "        Pr√©dictions (0 ou 1)\n",
    "    cost_fn : float\n",
    "        Co√ªt d'un Faux N√©gatif\n",
    "    cost_fp : float\n",
    "        Co√ªt d'un Faux Positif\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float : Co√ªt m√©tier total\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    # Matrice de confusion\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # Co√ªt total\n",
    "    total_cost = (fn * cost_fn) + (fp * cost_fp)\n",
    "\n",
    "    # Normaliser par le nombre d'exemples\n",
    "    cost_per_client = total_cost / len(y_true)\n",
    "\n",
    "    return cost_per_client\n",
    "\n",
    "\n",
    "def business_score_from_proba(y_true, y_proba, threshold=0.5, cost_fn=10, cost_fp=1):\n",
    "    \"\"\"\n",
    "    Calcule le score m√©tier √† partir de probabilit√©s et d'un seuil.\n",
    "    \"\"\"\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    return business_cost_score(y_true, y_pred, cost_fn, cost_fp)\n",
    "\n",
    "\n",
    "# Test du score\n",
    "print(\"\\nüìä Test du Score M√©tier:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sc√©nario 1 : Mod√®le parfait\n",
    "y_true_test = np.array([0, 0, 1, 1])\n",
    "y_pred_perfect = np.array([0, 0, 1, 1])\n",
    "cost_perfect = business_cost_score(y_true_test, y_pred_perfect, COST_FN, 1)\n",
    "print(f\"Mod√®le parfait : Co√ªt = {cost_perfect:.4f}\")\n",
    "\n",
    "# Sc√©nario 2 : 1 Faux N√©gatif\n",
    "y_pred_fn = np.array([0, 0, 0, 1])  # Manque 1 d√©faut\n",
    "cost_fn = business_cost_score(y_true_test, y_pred_fn, COST_FN, 1)\n",
    "print(f\"1 Faux N√©gatif : Co√ªt = {cost_fn:.4f} (= {COST_FN}/4 = {COST_FN / 4:.2f})\")\n",
    "\n",
    "# Sc√©nario 3 : 1 Faux Positif\n",
    "y_pred_fp = np.array([1, 0, 1, 1])  # Rejette 1 bon client\n",
    "cost_fp = business_cost_score(y_true_test, y_pred_fp, COST_FN, 1)\n",
    "print(f\"1 Faux Positif : Co√ªt = {cost_fp:.4f} (= 1/4 = {1 / 4:.2f})\")\n",
    "\n",
    "print(f\"\\nüí° Un FN co√ªte {cost_fn / cost_fp:.0f}x plus qu'un FP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbdf7a",
   "metadata": {},
   "source": [
    "### G√©n√®re un rapport business complet √† partir du score m√©tier CV.\n",
    "\n",
    "| Avantage         | D√©tail                                           |\n",
    "| ---------------- | ------------------------------------------------ |\n",
    "| **R√©utilisable** | Un seul appel pour g√©n√©rer tout le rapport       |\n",
    "| **Flexible**     | Param√®tres personnalisables (LGD, marge, volume) |\n",
    "| **Automatique**  | Charge les donn√©es et calcule tout seul          |\n",
    "| **Comparable**   | Fonction bonus pour comparer plusieurs mod√®les   |\n",
    "| **Int√©grable**   | Retourne un dict de KPIs pour MLFlow             |\n",
    "| **Sauvegard√©**   | G√©n√®re un fichier .txt horodat√©                  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6187de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# G√âN√âRATEUR DE RAPPORT BUSINESS AUTOMATIQUE\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def generer_rapport_business(\n",
    "    cv_business_cost,\n",
    "    amt_credit_mean=None,\n",
    "    taux_defaut=None,\n",
    "    # Param√®tres optionnels avec valeurs par d√©faut\n",
    "    lgd=0.75,\n",
    "    profit_margin=0.05,\n",
    "    volume_annuel=100_000,\n",
    "    projection_annees=3,\n",
    "    # Chemins\n",
    "    train_path=\"../data/train_aggregated.csv\",\n",
    "    output_dir=\"../reports\",\n",
    "    # Options\n",
    "    sauvegarder=True,\n",
    "    afficher=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    G√©n√®re un rapport business complet √† partir du score m√©tier CV.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cv_business_cost : float\n",
    "        Score m√©tier en cross-validation (ex: 0.4964)\n",
    "    amt_credit_mean : float, optional\n",
    "        Montant moyen du cr√©dit. Si None, calcul√© depuis train_path\n",
    "    taux_defaut : float, optional\n",
    "        Taux de d√©faut. Si None, calcul√© depuis train_path\n",
    "    lgd : float, default=0.75\n",
    "        Loss Given Default (75% = standard cr√©dit consommation)\n",
    "    profit_margin : float, default=0.05\n",
    "        Marge sur un bon client (5%)\n",
    "    volume_annuel : int, default=100000\n",
    "        Nombre de clients projet√©s par an\n",
    "    projection_annees : int, default=3\n",
    "        Nombre d'ann√©es pour la projection\n",
    "    train_path : str\n",
    "        Chemin vers train_aggregated.csv\n",
    "    output_dir : str\n",
    "        R√©pertoire de sortie pour le rapport\n",
    "    sauvegarder : bool, default=True\n",
    "        Sauvegarder le rapport en fichier txt\n",
    "    afficher : bool, default=True\n",
    "        Afficher le rapport dans le terminal\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionnaire avec tous les KPIs calcul√©s\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================\n",
    "    # 1. CHARGER LES DONN√âES SI N√âCESSAIRE\n",
    "    # ============================================\n",
    "\n",
    "    if amt_credit_mean is None or taux_defaut is None:\n",
    "        print(f\"üìÇ Chargement de {train_path}...\")\n",
    "        try:\n",
    "            train = pd.read_csv(train_path)\n",
    "\n",
    "            if amt_credit_mean is None:\n",
    "                amt_credit_mean = train[\"AMT_CREDIT\"].mean()\n",
    "\n",
    "            if taux_defaut is None:\n",
    "                if \"TARGET\" not in train.columns:\n",
    "                    raise ValueError(\"‚ùå Colonne TARGET absente. V√©rifiez le fichier.\")\n",
    "                taux_defaut = train[\"TARGET\"].mean()\n",
    "\n",
    "            n_clients_historique = len(train)\n",
    "            print(f\"‚úÖ Donn√©es charg√©es : {n_clients_historique:,} clients\\n\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\n",
    "                f\"‚ùå Fichier non trouv√© : {train_path}\\n\"\n",
    "                \"Veuillez fournir amt_credit_mean et taux_defaut manuellement.\"\n",
    "            )\n",
    "    else:\n",
    "        n_clients_historique = None\n",
    "\n",
    "    # ============================================\n",
    "    # 2. CALCULS DES CO√õTS UNITAIRES\n",
    "    # ============================================\n",
    "\n",
    "    cost_fn_euro = amt_credit_mean * lgd\n",
    "    cost_fp_euro = amt_credit_mean * profit_margin\n",
    "    ratio = cost_fn_euro / cost_fp_euro\n",
    "\n",
    "    # ============================================\n",
    "    # 3. CONVERSION DU SCORE EN EUROS\n",
    "    # ============================================\n",
    "\n",
    "    # Le score est normalis√© avec cost_fp = 1\n",
    "    # Donc : multiplier par cost_fp_euro pour avoir en ‚Ç¨\n",
    "    cout_modele_par_client = cv_business_cost * cost_fp_euro\n",
    "\n",
    "    # ============================================\n",
    "    # 4. BASELINE (MOD√àLE NA√èF)\n",
    "    # ============================================\n",
    "\n",
    "    cout_naif_par_client = taux_defaut * cost_fn_euro\n",
    "\n",
    "    # ============================================\n",
    "    # 5. √âCONOMIES\n",
    "    # ============================================\n",
    "\n",
    "    economie_par_client = cout_naif_par_client - cout_modele_par_client\n",
    "    amelioration_pct = (economie_par_client / cout_naif_par_client) * 100\n",
    "\n",
    "    # Impact annuel\n",
    "    economie_annuelle = economie_par_client * volume_annuel\n",
    "\n",
    "    # Projection multi-ann√©es\n",
    "    economie_projection = economie_annuelle * projection_annees\n",
    "\n",
    "    # ============================================\n",
    "    # 6. ESTIMATION R√âPARTITION FN/FP\n",
    "    # ============================================\n",
    "\n",
    "    # Sur 1000 clients\n",
    "    n_bons_1000 = int(1000 * (1 - taux_defaut))\n",
    "    n_mauvais_1000 = int(1000 * taux_defaut)\n",
    "\n",
    "    # Estimation inverse du nombre d'erreurs\n",
    "    # cv_business_cost √ó 1000 = (fn √ó 10) + (fp √ó 1)\n",
    "    total_cost_1000 = cv_business_cost * 1000\n",
    "\n",
    "    # Estimation simplifi√©e (assume un √©quilibre raisonnable)\n",
    "    # Ratio r√©el est 15:1, donc ajuster\n",
    "    fn_estimate = int(total_cost_1000 / (10 + ratio / 15))\n",
    "    fp_estimate = int((total_cost_1000 - fn_estimate * 10))\n",
    "\n",
    "    # ============================================\n",
    "    # 7. CONSTRUCTION DU RAPPORT\n",
    "    # ============================================\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    rapport = f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë          RAPPORT D'IMPACT BUSINESS - SCORING CR√âDIT                ‚ïë\n",
    "‚ïë          G√©n√©r√© le {timestamp}                       ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                          1. CONTEXTE                               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "  Donn√©es de Base:\n",
    "    ‚Ä¢ Montant moyen des cr√©dits : {amt_credit_mean:,.0f} ‚Ç¨\n",
    "    ‚Ä¢ Taux de d√©faut historique : {taux_defaut:.2%}\n",
    "\"\"\"\n",
    "\n",
    "    if n_clients_historique:\n",
    "        rapport += f\"    ‚Ä¢ √âchantillon analys√© : {n_clients_historique:,} clients\\n\"\n",
    "\n",
    "    rapport += f\"\"\"\n",
    "  Hypoth√®ses M√©tier:\n",
    "    ‚Ä¢ Loss Given Default (LGD) : {lgd:.0%}\n",
    "      ‚Üí Perte moyenne en cas de d√©faut\n",
    "    ‚Ä¢ Marge sur bon client : {profit_margin:.1%}\n",
    "      ‚Üí Profit moyen par client remboursant\n",
    "    ‚Ä¢ Volume projet√© : {volume_annuel:,} clients/an\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                   2. CO√õTS DES ERREURS                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "  üí• Faux N√©gatif (FN) - Accepter un mauvais client:\n",
    "    ‚Ä¢ Co√ªt unitaire : {cost_fn_euro:,.0f} ‚Ç¨\n",
    "    ‚Ä¢ Calcul : {amt_credit_mean:,.0f} ‚Ç¨ √ó {lgd:.0%} = {cost_fn_euro:,.0f} ‚Ç¨\n",
    "    ‚Ä¢ Impact : Perte du capital pr√™t√©\n",
    "\n",
    "  üòï Faux Positif (FP) - Refuser un bon client:\n",
    "    ‚Ä¢ Co√ªt unitaire : {cost_fp_euro:,.0f} ‚Ç¨\n",
    "    ‚Ä¢ Calcul : {amt_credit_mean:,.0f} ‚Ç¨ √ó {profit_margin:.1%} = {cost_fp_euro:,.0f} ‚Ç¨\n",
    "    ‚Ä¢ Impact : Manque √† gagner (marge perdue)\n",
    "\n",
    "  ‚öñÔ∏è  Ratio d'Importance:\n",
    "    ‚Ä¢ Un FN co√ªte {ratio:.1f}√ó plus qu'un FP\n",
    "    ‚Ä¢ Priorit√© : D√©tecter les mauvais clients\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                  3. PERFORMANCE DU MOD√àLE                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "  üìä Score M√©tier (Cross-Validation):\n",
    "    ‚Ä¢ Score normalis√© : {cv_business_cost:.4f}\n",
    "    ‚Ä¢ Co√ªt r√©el par client : {cout_modele_par_client:,.0f} ‚Ç¨\n",
    "\n",
    "  üìâ Baseline (Accepter tous les clients):\n",
    "    ‚Ä¢ Co√ªt par client : {cout_naif_par_client:,.0f} ‚Ç¨\n",
    "    ‚Ä¢ Calcul : {taux_defaut:.2%} √ó {cost_fn_euro:,.0f} ‚Ç¨ = {cout_naif_par_client:,.0f} ‚Ç¨\n",
    "\n",
    "  ‚úÖ Am√©lioration:\n",
    "    ‚Ä¢ R√©duction du co√ªt : {amelioration_pct:.1f}%\n",
    "    ‚Ä¢ √âconomie par client : {economie_par_client:,.0f} ‚Ç¨\n",
    "\n",
    "  üîç Estimation des Erreurs (sur 1000 clients):\n",
    "    ‚Ä¢ {n_mauvais_1000} clients √† risque ‚Üí ~{fn_estimate} accept√©s (FN)\n",
    "    ‚Ä¢ {n_bons_1000} bons clients ‚Üí ~{fp_estimate} refus√©s (FP)\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    4. IMPACT FINANCIER                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "  üí∞ Par Client:\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ                     ‚îÇ  Sans Mod√®le     ‚îÇ  Avec Mod√®le     ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "    ‚îÇ Co√ªt par client     ‚îÇ {cout_naif_par_client:>13,.0f} ‚Ç¨ ‚îÇ {cout_modele_par_client:>13,.0f} ‚Ç¨ ‚îÇ\n",
    "    ‚îÇ √âconomie            ‚îÇ         -        ‚îÇ {economie_par_client:>13,.0f} ‚Ç¨ ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "  üìà Impact Annuel ({volume_annuel:,} clients):\n",
    "    ‚Ä¢ Co√ªt sans mod√®le : {cout_naif_par_client * volume_annuel:,.0f} ‚Ç¨\n",
    "      = {(cout_naif_par_client * volume_annuel) / 1_000_000:.2f} millions ‚Ç¨\n",
    "\n",
    "    ‚Ä¢ Co√ªt avec mod√®le : {cout_modele_par_client * volume_annuel:,.0f} ‚Ç¨\n",
    "      = {(cout_modele_par_client * volume_annuel) / 1_000_000:.2f} millions ‚Ç¨\n",
    "\n",
    "    ‚Ä¢ üí∞ √âCONOMIE ANNUELLE : {economie_annuelle:,.0f} ‚Ç¨\n",
    "                           = {economie_annuelle / 1_000_000:.2f} millions ‚Ç¨\n",
    "\n",
    "  üéØ Projection {projection_annees} ans:\n",
    "\"\"\"\n",
    "\n",
    "    for annee in range(1, projection_annees + 1):\n",
    "        economie_cumul = economie_annuelle * annee\n",
    "        rapport += f\"    ‚Ä¢ Ann√©e {annee} (cumul√©) : {economie_cumul / 1_000_000:.2f} millions ‚Ç¨\\n\"\n",
    "\n",
    "    rapport += f\"\"\"\n",
    "    ‚Ä¢ üèÜ TOTAL {projection_annees} ANS : {economie_projection:,.0f} ‚Ç¨\n",
    "                     = {economie_projection / 1_000_000:.2f} millions ‚Ç¨\n",
    "                     = {economie_projection / 1_000_000_000:.2f} milliards ‚Ç¨\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                      5. CONCLUSION                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "  Le mod√®le de machine learning permet de r√©duire le co√ªt par client\n",
    "  de {amelioration_pct:.0f}%, en d√©tectant mieux les clients √† risque tout en\n",
    "  maintenant un taux d'acceptation raisonnable des bons clients.\n",
    "\n",
    "  ROI: Pour chaque {volume_annuel:,} clients trait√©s par an, le mod√®le\n",
    "       √©conomise {economie_annuelle / 1_000_000:.2f} millions d'euros.\n",
    "\n",
    "  Recommandation: ‚úÖ D√©ployer le mod√®le en production\n",
    "\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "\n",
    "    # ============================================\n",
    "    # 8. AFFICHAGE\n",
    "    # ============================================\n",
    "\n",
    "    if afficher:\n",
    "        print(rapport)\n",
    "\n",
    "    # ============================================\n",
    "    # 9. SAUVEGARDE\n",
    "    # ============================================\n",
    "\n",
    "    if sauvegarder:\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        filename = f\"rapport_business_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "        filepath = output_path / filename\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(rapport)\n",
    "\n",
    "        print(f\"\\n‚úÖ Rapport sauvegard√© : {filepath}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 10. RETOUR DES KPIs\n",
    "    # ============================================\n",
    "\n",
    "    kpis = {\n",
    "        # Entr√©es\n",
    "        \"cv_business_cost\": cv_business_cost,\n",
    "        \"amt_credit_mean\": amt_credit_mean,\n",
    "        \"taux_defaut\": taux_defaut,\n",
    "        \"lgd\": lgd,\n",
    "        \"profit_margin\": profit_margin,\n",
    "        \"volume_annuel\": volume_annuel,\n",
    "        # Co√ªts unitaires\n",
    "        \"cost_fn_euro\": cost_fn_euro,\n",
    "        \"cost_fp_euro\": cost_fp_euro,\n",
    "        \"ratio_fn_fp\": ratio,\n",
    "        # Performance\n",
    "        \"cout_modele_par_client\": cout_modele_par_client,\n",
    "        \"cout_naif_par_client\": cout_naif_par_client,\n",
    "        \"economie_par_client\": economie_par_client,\n",
    "        \"amelioration_pct\": amelioration_pct,\n",
    "        # Impact\n",
    "        \"economie_annuelle\": economie_annuelle,\n",
    "        \"economie_projection\": economie_projection,\n",
    "        # Estimation erreurs\n",
    "        \"fn_estimate_per_1000\": fn_estimate,\n",
    "        \"fp_estimate_per_1000\": fp_estimate,\n",
    "    }\n",
    "\n",
    "    return kpis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fbed7",
   "metadata": {},
   "source": [
    "## Fonction de train et de log des modeles pour selectionner le modele pour le grid search et la recherche d'hyperparametres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bf72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(model, model_name, X_train, y_train, params=None):\n",
    "    \"\"\"Entra√Æne un mod√®le et enregistre les m√©triques et artefacts dans MLflow.\n",
    "\n",
    "    Args:\n",
    "        model (_type_): Mod√®le √† entra√Æner\n",
    "        model_name (_type_): Nom du mod√®le\n",
    "        X_train (_type_): Donn√©es d'entra√Ænement\n",
    "        y_train (_type_): Cibles d'entra√Ænement\n",
    "        params (_type_, optional): Param√®tres du mod√®le. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Co√ªt m√©tier moyen sur la validation crois√©e\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # --- 1. Param√®tres & M√©tadonn√©es ---\n",
    "        if params:\n",
    "            mlflow.log_params(params)\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_param(\"train_shape\", str(X_train.shape))\n",
    "        mlflow.log_param(\n",
    "            \"target_dist\", str(y_train.value_counts(normalize=True).to_dict())\n",
    "        )\n",
    "\n",
    "        # --- 2. Cross-Validation (Technique + M√©tier) ---\n",
    "        scoring = {\n",
    "            \"auc\": \"roc_auc\",\n",
    "            \"business\": make_scorer(\n",
    "                business_cost_score, cost_fn=COST_FN, cost_fp=1, greater_is_better=False\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        cv_results = cross_validate(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=StratifiedKFold(\n",
    "                n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE\n",
    "            ),\n",
    "            scoring=scoring,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        # --- 3. Entra√Ænement final & Pr√©dictions ---\n",
    "        model.fit(X_train, y_train)\n",
    "        y_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "        # On utilise le seuil par d√©faut 0.5 pour la baseline\n",
    "        y_pred_05 = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "        # --- 4. Calcul des m√©triques ---\n",
    "        train_auc = roc_auc_score(y_train, y_proba)\n",
    "        cv_auc_mean = cv_results[\"test_auc\"].mean()\n",
    "\n",
    "        # Matrice de confusion (pour extraire TN, FP, FN, TP)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_train, y_pred_05).ravel()\n",
    "\n",
    "        # --- 5. LOGS MLFLOW  ---\n",
    "        # CV Metrics\n",
    "        mlflow.log_metric(\"cv_auc_mean\", cv_auc_mean)\n",
    "        mlflow.log_metric(\"cv_auc_std\", cv_results[\"test_auc\"].std())\n",
    "\n",
    "        # ‚ö†Ô∏è IMPORTANT: greater_is_better=False fait que sklearn INVERSE le signe\n",
    "        # cv_results[\"test_business\"] contient des valeurs N√âGATIVES\n",
    "        # On re-inverse avec `-` pour avoir un co√ªt POSITIF dans MLFlow\n",
    "        mlflow.log_metric(\"cv_business_cost_mean\", -cv_results[\"test_business\"].mean())\n",
    "\n",
    "        # Overfitting Gaps\n",
    "        mlflow.log_metric(\"auc_gap\", train_auc - cv_auc_mean)\n",
    "\n",
    "        # Confusion Matrix Metrics\n",
    "        mlflow.log_metric(\"train_tn\", int(tn))\n",
    "        mlflow.log_metric(\"train_fp\", int(fp))\n",
    "        mlflow.log_metric(\"train_fn\", int(fn))\n",
    "        mlflow.log_metric(\"train_tp\", int(tp))\n",
    "\n",
    "        # --- 6. Artefacts (Plots & Mod√®le) ---\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        sns.heatmap(\n",
    "            confusion_matrix(y_train, y_pred_05), annot=True, fmt=\"d\", cmap=\"Blues\"\n",
    "        )\n",
    "        mlflow.log_figure(fig, \"confusion_matrix_default.png\")\n",
    "        plt.close()\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        print(\n",
    "            f\"‚úÖ {model_name} | CV AUC: {cv_auc_mean:.4f} | Gap: {train_auc - cv_auc_mean:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Re-inverse pour retourner le co√ªt positif (sklearn l'a invers√©)\n",
    "        return -cv_results[\"test_business\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9484e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ D√âMARRAGE DES ENTRA√éNEMENTS BASELINES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/13 16:14:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/13 16:14:08 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logistic_Regression_Baseline_balanced | CV AUC: 0.7705 | Gap: 0.0057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/13 16:14:21 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/13 16:14:23 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logistic_Regression_Baseline_non_balanced | CV AUC: 0.7700 | Gap: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/13 16:15:59 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/13 16:16:00 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RandomForest_Baseline | CV AUC: 0.7545 | Gap: 0.0788\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46867\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 401\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.448147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46893\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 402\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180919\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180919\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46847\n",
      "[LightGBM] [Info] Number of data points in the train set: 196807, number of used features: 401\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.350458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46834\n",
      "[LightGBM] [Info] Number of data points in the train set: 196807, number of used features: 401\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.640001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 46868\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 402\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46889\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 403\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/13 16:16:37 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/13 16:16:38 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LightGBM_Baseline_balanced | CV AUC: 0.7785 | Gap: 0.0442\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.467164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46868\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 402\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432480\n",
      "[LightGBM] [Info] Start training from score -2.432480\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180919\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.309859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46834\n",
      "[LightGBM] [Info] Number of data points in the train set: 196807, number of used features: 401\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432485\n",
      "[LightGBM] [Info] Start training from score -2.432485\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.247219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46893\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 402\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432480\n",
      "[LightGBM] [Info] Start training from score -2.432480\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.605305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 46847\n",
      "[LightGBM] [Info] Number of data points in the train set: 196807, number of used features: 401\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432485\n",
      "[LightGBM] [Info] Start training from score -2.432485\n",
      "[LightGBM] [Info] Number of positive: 15888, number of negative: 180918\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.256283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46867\n",
      "[LightGBM] [Info] Number of data points in the train set: 196806, number of used features: 401\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432480\n",
      "[LightGBM] [Info] Start training from score -2.432480\n",
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46889\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 403\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432482\n",
      "[LightGBM] [Info] Start training from score -2.432482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/13 16:17:14 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/13 16:17:16 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LightGBM_Baseline_non_balanced | CV AUC: 0.7782 | Gap: 0.0460\n",
      "\n",
      "============================================================\n",
      "üìä R√âSULTATS FINAUX\n",
      "============================================================\n",
      "                                    Model  CV Business Cost\n",
      "               LightGBM_Baseline_balanced          0.499955\n",
      "    Logistic_Regression_Baseline_balanced          0.511394\n",
      "                    RandomForest_Baseline          0.535344\n",
      "           LightGBM_Baseline_non_balanced          0.784300\n",
      "Logistic_Regression_Baseline_non_balanced          0.787718\n",
      "\n",
      "üí° Le meilleur mod√®le est : LightGBM_Baseline_balanced\n"
     ]
    }
   ],
   "source": [
    "# D√©finition des mod√®les √† tester\n",
    "models_to_test = [\n",
    "    {\n",
    "        \"name\": \"Logistic_Regression_Baseline_balanced\",\n",
    "        \"model\": LogisticRegression(\n",
    "            max_iter=1000, random_state=RANDOM_STATE, class_weight=\"balanced\"\n",
    "        ),\n",
    "        \"params\": {\"max_iter\": 1000, \"class_weight\": \"balanced\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic_Regression_Baseline_non_balanced\",\n",
    "        \"model\": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "        \"params\": {\"max_iter\": 1000},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RandomForest_Baseline\",\n",
    "        \"model\": RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "        \"params\": {\"n_estimators\": 100, \"max_depth\": 10},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LightGBM_Baseline_balanced\",\n",
    "        \"model\": lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=7,\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight=\"balanced\",\n",
    "        ),\n",
    "        \"params\": {\"n_estimators\": 100, \"learning_rate\": 0.1},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LightGBM_Baseline_non_balanced\",\n",
    "        \"model\": lgb.LGBMClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=7, random_state=RANDOM_STATE\n",
    "        ),\n",
    "        \"params\": {\"n_estimators\": 100, \"learning_rate\": 0.1},\n",
    "    },\n",
    "]\n",
    "\n",
    "# Ex√©cution de la boucle\n",
    "results_list = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ D√âMARRAGE DES ENTRA√éNEMENTS BASELINES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for m in models_to_test:\n",
    "    score = train_and_log_model(\n",
    "        m[\"model\"], m[\"name\"], X_train, y_train, params=m[\"params\"]\n",
    "    )\n",
    "    results_list.append({\"Model\": m[\"name\"], \"CV Business Cost\": score})\n",
    "\n",
    "# 3. Comparaison finale\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä R√âSULTATS FINAUX\")\n",
    "print(\"=\" * 60)\n",
    "baseline_results = pd.DataFrame(results_list).sort_values(\"CV Business Cost\")\n",
    "print(baseline_results.to_string(index=False))\n",
    "\n",
    "best_baseline = baseline_results.iloc[0][\"Model\"]\n",
    "print(f\"\\nüí° Le meilleur mod√®le est : {best_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6491734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Hyperparameter Tuning sur LightGBM_Baseline_balanced\n",
      "============================================================\n",
      "‚è≥ GridSearch en cours...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 3.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 3.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 3.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 3.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.01, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time= 1.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time=  58.6s\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time=  58.9s\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time=  59.2s\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time=  52.0s\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time=  52.7s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time=  53.8s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time=  54.3s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time=  54.4s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=31, verbosity=-1; total time=  55.8s\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.9min\n",
      "[CV] END force_row_wise=True, learning_rate=0.05, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 2.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time=  55.4s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time=  55.4s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time=  59.9s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.0min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time=  41.3s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time=  50.2s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time=  47.5s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time=  48.8s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=20, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=31, verbosity=-1; total time=  45.9s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time=  48.5s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.5min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time=  56.1s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time=  55.3s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time=  56.7s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time=  57.4s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=31, verbosity=-1; total time=  57.0s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time=  59.9s\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=50, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=50, verbosity=-1; total time= 1.1min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=100, num_leaves=70, verbosity=-1; total time= 1.2min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=31, verbosity=-1; total time= 1.4min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.6min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=50, verbosity=-1; total time= 1.7min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.8min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.3min\n",
      "[CV] END force_row_wise=True, learning_rate=0.1, min_child_samples=100, n_estimators=200, num_leaves=70, verbosity=-1; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/13 17:04:45 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/13 17:04:46 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Tuning termin√© et loggu√© de fa√ßon exhaustive.\n",
      "Meilleurs param√®tres: {'force_row_wise': True, 'learning_rate': 0.05, 'min_child_samples': 50, 'n_estimators': 200, 'num_leaves': 50, 'verbosity': -1}\n",
      "Meilleur CV AUC: 0.7814 | CV Business Cost: 0.4951\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## 4. Hyperparameter Tuning avec GridSearchCV (Harmonis√©)\n",
    "\n",
    "On optimise le meilleur mod√®le baseline avec GridSearchCV.\n",
    "On utilise le multi-scoring pour logger l'AUC et le Score M√©tier en m√™me temps.\n",
    "\"\"\"\n",
    "\n",
    "best_baseline_name = \"LightGBM_Baseline_balanced\"\n",
    "\n",
    "print(f\"\\nüîç Hyperparameter Tuning sur {best_baseline_name}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. D√©finition des scorers (AUC + M√©tier)\n",
    "scoring = {\n",
    "    \"AUC\": \"roc_auc\",\n",
    "    \"Business_Score\": make_scorer(\n",
    "        business_cost_score, greater_is_better=False, cost_fn=COST_FN, cost_fp=1\n",
    "    ),\n",
    "}\n",
    "\n",
    "# 2. Grille de param√®tres\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"num_leaves\": [31, 50, 70],  # contr√¥le la complexit√©\n",
    "    \"min_child_samples\": [20, 50, 100],  # r√©gularisation pour le d√©s√©quilibre\n",
    "    \"force_row_wise\": [True],\n",
    "    \"verbosity\": [-1],\n",
    "}\n",
    "\n",
    "# LightGBM utilise des arbres leaf-wise. La r√®gle est : num_leaves <= 2^max_depth. Or ta grille teste :\n",
    "\n",
    "# max_depth=5 + num_leaves=70 ‚Üí 70 > 2^5 = 32, donc num_leaves est ignor√© silencieusement\n",
    "# max_depth=5 + num_leaves=50 ‚Üí idem\n",
    "# √áa veut dire que tu gaspilles du temps de calcul sur des combinaisons qui reviennent au m√™me. Deux options :\n",
    "\n",
    "# Retirer max_depth et ne garder que num_leaves (approche recommand√©e LightGBM)\n",
    "# Ou garder max_depth et ajuster num_leaves pour qu'il soit coh√©rent\n",
    "\n",
    "\n",
    "lgbm_grid = lgb.LGBMClassifier(\n",
    "    random_state=RANDOM_STATE, class_weight=\"balanced\", n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    lgbm_grid,  # type:ignore\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring=scoring,\n",
    "    refit=\"Business_Score\",  # Crucial: On choisit le meilleur mod√®le selon le co√ªt m√©tier\n",
    "    return_train_score=True,  # Pour calculer les gaps d'overfitting\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "print(\"‚è≥ GridSearch en cours...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- 4. LOG HARMONIS√â DANS MLFLOW ---\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{best_baseline_name}_Tuned_Complete\"):\n",
    "    # Index du meilleur run\n",
    "    idx = grid_search.best_index_\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # A. Param√®tres\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "    mlflow.log_param(\"model_type\", f\"{best_baseline_name}_Tuned\")\n",
    "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "\n",
    "    # B. M√©triques de Cross-Validation (Moyennes et √âcarts-types)\n",
    "    cv_auc_mean = grid_search.cv_results_[\"mean_test_AUC\"][idx]\n",
    "    cv_cost_mean = -grid_search.cv_results_[\"mean_test_Business_Score\"][idx]\n",
    "\n",
    "    mlflow.log_metric(\"cv_auc_mean\", cv_auc_mean)\n",
    "    mlflow.log_metric(\"cv_auc_std\", grid_search.cv_results_[\"std_test_AUC\"][idx])\n",
    "    mlflow.log_metric(\"cv_business_cost_mean\", cv_cost_mean)\n",
    "    mlflow.log_metric(\n",
    "        \"cv_business_cost_std\", grid_search.cv_results_[\"std_test_Business_Score\"][idx]\n",
    "    )\n",
    "\n",
    "    # C. M√©triques sur le Train (pour d√©tection Overfitting)\n",
    "    y_proba = best_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_05 = (y_proba >= 0.5).astype(int)  # Seuil baseline\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, y_proba)\n",
    "\n",
    "    # D. Gaps & Confusion Matrix\n",
    "    mlflow.log_metric(\"auc_gap\", train_auc - cv_auc_mean)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred_05).ravel()\n",
    "    mlflow.log_metric(\"train_fn\", int(fn))\n",
    "    mlflow.log_metric(\"train_fp\", int(fp))\n",
    "\n",
    "    # E. Artefacts\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(confusion_matrix(y_train, y_pred_05), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    ax.set_title(f\"Confusion Matrix - {best_baseline_name} Tuned (Seuil 0.5)\")\n",
    "    mlflow.log_figure(fig, \"confusion_matrix_tuned.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # F. Mod√®le\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Tuning termin√© et loggu√© de fa√ßon exhaustive.\")\n",
    "    print(f\"Meilleurs param√®tres: {grid_search.best_params_}\")\n",
    "    print(f\"Meilleur CV AUC: {cv_auc_mean:.4f} | CV Business Cost: {cv_cost_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d40013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Optimisation du seuil en 5-fold CV...\n",
      "  Fold 1/5 termin√©\n",
      "  Fold 2/5 termin√©\n",
      "  Fold 3/5 termin√©\n",
      "  Fold 4/5 termin√©\n",
      "  Fold 5/5 termin√©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/13 17:07:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/13 17:07:42 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "2026/02/13 17:07:42 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/02/13 17:07:42 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/02/13 17:07:42 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/13 17:07:42 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le Final loggu√© et enregistr√© !\n",
      "Score CV m√©tier: 0.4951 (¬± 0.0068)\n",
      "Gain m√©tier sur train (seuil 0.5 -> 0.494): 0.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'Credit_Scoring_Model_LGBM'.\n",
      "Created version '1' of model 'Credit_Scoring_Model_LGBM'.\n"
     ]
    }
   ],
   "source": [
    "def find_optimal_threshold(y_true, y_proba, cost_fn=10, cost_fp=1):\n",
    "    \"\"\"\n",
    "    Trouve le seuil optimal.\n",
    "    Note: On √©largit l'intervalle car avec FN=10, le seuil est souvent bas.\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0.02, 0.7, 100)  # Plus de pr√©cision sur les seuils bas\n",
    "    costs = []\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_proba >= thresh).astype(int)\n",
    "        cost = business_cost_score(y_true, y_pred, cost_fn, cost_fp)\n",
    "        costs.append(cost)\n",
    "\n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_cost = costs[optimal_idx]\n",
    "\n",
    "    # Figure d'optimisation\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(thresholds, costs, color=\"navy\", lw=2)\n",
    "    ax.axvline(\n",
    "        optimal_threshold,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Optimal: {optimal_threshold:.3f}\",\n",
    "    )\n",
    "    ax.axvline(0.5, color=\"orange\", linestyle=\":\", label=\"D√©faut: 0.500\")\n",
    "    ax.set_title(\"Recherche du Seuil Minimisant le Co√ªt M√©tier\")\n",
    "    ax.set_xlabel(\"Seuil\")\n",
    "    ax.set_ylabel(\"Co√ªt M√©tier\")\n",
    "    ax.legend()\n",
    "\n",
    "    return optimal_threshold, optimal_cost, fig\n",
    "\n",
    "\n",
    "def find_optimal_threshold_cv(\n",
    "    model, X_train, y_train, cost_fn=10, cost_fp=1, n_folds=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Version CROSS-VALIDATION - √©vite l'overfitting du seuil.\n",
    "    Utilise Out-Of-Fold predictions.\n",
    "    \"\"\"\n",
    "    from sklearn.base import clone\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    y_proba_oof = np.zeros(len(y_train))\n",
    "\n",
    "    print(f\"üîÑ Optimisation du seuil en {n_folds}-fold CV...\")\n",
    "\n",
    "    # G√©n√©rer les pr√©dictions OOF\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        X_vl = X_train.iloc[val_idx]\n",
    "\n",
    "        # Cloner et entra√Æner\n",
    "        model_fold = clone(model)\n",
    "        model_fold.fit(X_tr, y_tr)\n",
    "\n",
    "        # Pr√©dire sur validation fold\n",
    "        y_proba_oof[val_idx] = model_fold.predict_proba(X_vl)[:, 1]\n",
    "\n",
    "        print(f\"  Fold {fold_idx + 1}/{n_folds} termin√©\")\n",
    "\n",
    "    # Chercher le seuil optimal sur OOF\n",
    "    thresholds = np.linspace(0.02, 0.7, 100)\n",
    "    costs = []\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_proba_oof >= thresh).astype(int)\n",
    "        cost = business_cost_score(y_train, y_pred, cost_fn, cost_fp)\n",
    "        costs.append(cost)\n",
    "\n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_cost = costs[optimal_idx]\n",
    "\n",
    "    # Figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(thresholds, costs, color=\"navy\", lw=2)\n",
    "    ax.axvline(\n",
    "        optimal_threshold,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Optimal (CV): {optimal_threshold:.3f}\",\n",
    "    )\n",
    "    ax.axvline(0.5, color=\"orange\", linestyle=\":\", label=\"D√©faut: 0.500\")\n",
    "    ax.set_title(\"Optimisation du Seuil en Cross-Validation (Sans Fuite)\")\n",
    "    ax.set_xlabel(\"Seuil\")\n",
    "    ax.set_ylabel(\"Co√ªt M√©tier\")\n",
    "    ax.legend()\n",
    "    plt.close()\n",
    "\n",
    "    return optimal_threshold, optimal_cost, fig, y_proba_oof\n",
    "\n",
    "\n",
    "# --- Ex√©cution ---\n",
    "\n",
    "# ‚úÖ CORRECTION : G√©n√©ration des pr√©dictions OOF pour √©viter le data leakage\n",
    "optimal_thresh, optimal_cost, fig_thresh, y_proba_oof = find_optimal_threshold_cv(\n",
    "    best_model, X_train, y_train, COST_FN, 1, N_FOLDS\n",
    ")\n",
    "\n",
    "# ‚úÖ CORRECTION : Utiliser y_proba_oof au lieu de y_train_proba\n",
    "y_pred_opt = (y_proba_oof >= optimal_thresh).astype(int)\n",
    "y_pred_def = (y_proba_oof >= 0.5).astype(int)\n",
    "\n",
    "# --- LOG FINAL HARMONIS√â ---\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{best_baseline}_FINAL_MODEL\"):\n",
    "    # 1. Hyperparam√®tres et Seuil\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "    mlflow.log_param(\"optimal_threshold\", round(optimal_thresh, 3))\n",
    "    mlflow.log_param(\"model_type\", f\"{best_baseline}_Tuned_Final\")\n",
    "\n",
    "    # 2. M√©triques de Cross-Validation (Harmonisation avec les Baselines)\n",
    "    # On r√©cup√®re les r√©sultats du GridSearch pour le meilleur index\n",
    "    best_idx = grid_search.best_index_\n",
    "\n",
    "    cv_auc = grid_search.cv_results_[\"mean_test_AUC\"][best_idx]\n",
    "    cv_cost = -grid_search.cv_results_[\"mean_test_Business_Score\"][\n",
    "        best_idx\n",
    "    ]  # On repasse en positif\n",
    "    cv_cost_std = grid_search.cv_results_[\"std_test_Business_Score\"][best_idx]\n",
    "\n",
    "    mlflow.log_metric(\"cv_auc_mean\", cv_auc)\n",
    "    mlflow.log_metric(\"cv_business_cost_mean\", cv_cost)\n",
    "    mlflow.log_metric(\"cv_business_cost_std\", cv_cost_std)\n",
    "\n",
    "    # 3. M√©triques de performance sur le Train (Technique)\n",
    "    # ‚úÖ CORRECTION : Calcul de train_auc sur OOF pour coh√©rence\n",
    "    train_auc = roc_auc_score(y_train, y_proba_oof)\n",
    "    mlflow.log_metric(\"train_auc\", train_auc)\n",
    "    mlflow.log_metric(\"auc_gap\", train_auc - cv_auc)\n",
    "\n",
    "    # 4. Impact M√©tier du Seuil (La valeur ajout√©e de cette √©tape)\n",
    "    cost_def = business_cost_score(y_train, y_pred_def, COST_FN, 1)\n",
    "    mlflow.log_metric(\"train_business_cost_at_05\", cost_def)\n",
    "    mlflow.log_metric(\"train_business_cost_optimal\", optimal_cost)\n",
    "\n",
    "    improvement = ((cost_def - optimal_cost) / cost_def) * 100\n",
    "    mlflow.log_metric(\"threshold_improvement_percent\", improvement)\n",
    "\n",
    "    # 5. Confusion Matrix au seuil OPTIMAL\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred_opt).ravel()\n",
    "    mlflow.log_metric(\"final_train_fn\", int(fn))\n",
    "    mlflow.log_metric(\"final_train_fp\", int(fp))\n",
    "    mlflow.log_metric(\"final_train_tp\", int(tp))\n",
    "    mlflow.log_metric(\"final_train_tn\", int(tn))\n",
    "\n",
    "    # 6. Artefacts\n",
    "    mlflow.log_figure(fig_thresh, \"threshold_optimization_curve.png\")\n",
    "\n",
    "    fig_cm, ax = plt.subplots()\n",
    "    sns.heatmap(\n",
    "        confusion_matrix(y_train, y_pred_opt), annot=True, fmt=\"d\", cmap=\"Greens\"\n",
    "    )\n",
    "    ax.set_title(f\"Matrice de Confusion au Seuil Optimal ({optimal_thresh:.3f})\")\n",
    "    mlflow.log_figure(fig_cm, \"final_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # 7. Model Registry\n",
    "    mlflow.sklearn.log_model(\n",
    "        best_model, \"model\", registered_model_name=\"Credit_Scoring_Model_LGBM\"\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Mod√®le Final loggu√© et enregistr√© !\")\n",
    "    print(f\"Score CV m√©tier: {cv_cost:.4f} (¬± {cv_cost_std:.4f})\")\n",
    "    print(\n",
    "        f\"Gain m√©tier sur train (seuil 0.5 -> {optimal_thresh:.3f}): {improvement:.1f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9e05q8qosq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ √âVALUATION FINALE SUR LE SET DE VALIDATION (20%)\n",
      "============================================================\n",
      "\n",
      "üìä AUC Validation : 0.7852\n",
      "üìä Business Cost (seuil 0.5)     : 0.4908\n",
      "üìä Business Cost (seuil 0.494) : 0.4907\n",
      "\n",
      "üìä Matrice de confusion (seuil 0.494) :\n",
      "   TN=41,659  FP=14,879\n",
      "   FN=1,530  TP=3,435\n",
      "\n",
      "üîç Comparaison CV vs Validation :\n",
      "   CV AUC          : 0.7814\n",
      "   Val AUC         : 0.7852\n",
      "   √âcart           : 0.0038\n",
      "   CV Business Cost: 0.4951\n",
      "   Val Business Cost: 0.4907\n",
      "\n",
      "‚úÖ Les scores CV et validation sont coh√©rents ‚Äî le mod√®le g√©n√©ralise bien.\n",
      "\n",
      "‚úÖ √âvaluation validation logu√©e dans MLflow\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## 5. Evaluation finale sur le set de validation (X_val)\n",
    "\n",
    "Ce set a √©t√© isol√© d√®s le d√©but (20% des donn√©es) et n'a jamais √©t√©\n",
    "utilis√© pendant l'entra√Ænement ni le tuning. C'est notre estimation\n",
    "honn√™te de la performance en production.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ √âVALUATION FINALE SUR LE SET DE VALIDATION (20%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pr√©dictions sur X_val\n",
    "y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_optimal = (y_val_proba >= optimal_thresh).astype(int)\n",
    "y_val_pred_default = (y_val_proba >= 0.5).astype(int)\n",
    "\n",
    "# M√©triques\n",
    "val_auc = roc_auc_score(y_val, y_val_proba)\n",
    "val_cost_optimal = business_cost_score(y_val, y_val_pred_optimal, COST_FN, 1)\n",
    "val_cost_default = business_cost_score(y_val, y_val_pred_default, COST_FN, 1)\n",
    "\n",
    "# Confusion matrix au seuil optimal\n",
    "tn_val, fp_val, fn_val, tp_val = confusion_matrix(y_val, y_val_pred_optimal).ravel()\n",
    "\n",
    "print(f\"\\nüìä AUC Validation : {val_auc:.4f}\")\n",
    "print(f\"üìä Business Cost (seuil 0.5)     : {val_cost_default:.4f}\")\n",
    "print(f\"üìä Business Cost (seuil {optimal_thresh:.3f}) : {val_cost_optimal:.4f}\")\n",
    "print(f\"\\nüìä Matrice de confusion (seuil {optimal_thresh:.3f}) :\")\n",
    "print(f\"   TN={tn_val:,}  FP={fp_val:,}\")\n",
    "print(f\"   FN={fn_val:,}  TP={tp_val:,}\")\n",
    "\n",
    "# Comparaison CV vs Validation\n",
    "print(f\"\\nüîç Comparaison CV vs Validation :\")\n",
    "print(f\"   CV AUC          : {cv_auc:.4f}\")\n",
    "print(f\"   Val AUC         : {val_auc:.4f}\")\n",
    "print(f\"   √âcart           : {abs(cv_auc - val_auc):.4f}\")\n",
    "print(f\"   CV Business Cost: {cv_cost:.4f}\")\n",
    "print(f\"   Val Business Cost: {val_cost_optimal:.4f}\")\n",
    "\n",
    "if abs(cv_auc - val_auc) < 0.02:\n",
    "    print(\n",
    "        \"\\n‚úÖ Les scores CV et validation sont coh√©rents ‚Äî le mod√®le g√©n√©ralise bien.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è √âcart significatif entre CV et validation ‚Äî possible overfitting.\")\n",
    "\n",
    "# Log dans MLflow\n",
    "with mlflow.start_run(run_name=f\"{best_baseline}_VALIDATION\"):\n",
    "    mlflow.log_metric(\"val_auc\", val_auc)\n",
    "    mlflow.log_metric(\"val_business_cost_optimal\", val_cost_optimal)\n",
    "    mlflow.log_metric(\"val_business_cost_default\", val_cost_default)\n",
    "    mlflow.log_metric(\"val_fn\", int(fn_val))\n",
    "    mlflow.log_metric(\"val_fp\", int(fp_val))\n",
    "    mlflow.log_metric(\"val_tn\", int(tn_val))\n",
    "    mlflow.log_metric(\"val_tp\", int(tp_val))\n",
    "    mlflow.log_param(\"optimal_threshold\", round(optimal_thresh, 3))\n",
    "    mlflow.log_param(\"model_type\", f\"{best_baseline}_Validation\")\n",
    "\n",
    "    fig_val, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        confusion_matrix(y_val, y_val_pred_optimal), annot=True, fmt=\"d\", cmap=\"Purples\"\n",
    "    )\n",
    "    ax.set_title(f\"Validation - Confusion Matrix (Seuil {optimal_thresh:.3f})\")\n",
    "    mlflow.log_figure(fig_val, \"validation_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n‚úÖ √âvaluation validation logu√©e dans MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20f65846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä COMPARAISON FINALE DES MOD√àLES\n",
      "============================================================\n",
      "\n",
      "Top 10 Mod√®les (tri√©s par co√ªt m√©tier):\n",
      "                      tags.mlflow.runName  metrics.cv_business_cost_mean  metrics.train_auc  metrics.cv_auc_mean  metrics.auc_gap\n",
      "   LightGBM_Baseline_balanced_FINAL_MODEL                       0.495069           0.781337             0.781354        -0.000016\n",
      "LightGBM_Baseline_balanced_Tuned_Complete                       0.495069                NaN             0.781354         0.070313\n",
      "LightGBM_Baseline_balanced_Tuned_Complete                       0.498224                NaN             0.779327         0.044570\n",
      "               LightGBM_Baseline_balanced                       0.499955                NaN             0.778513         0.044177\n",
      "               LightGBM_Baseline_balanced                       0.499955                NaN             0.778513         0.044177\n",
      "    Logistic_Regression_Baseline_balanced                       0.511394                NaN             0.770536         0.005719\n",
      "    Logistic_Regression_Baseline_balanced                       0.511394                NaN             0.770536         0.005719\n",
      "                    RandomForest_Baseline                       0.535344                NaN             0.754547         0.078766\n",
      "                    RandomForest_Baseline                       0.535344                NaN             0.754547         0.078766\n",
      "           LightGBM_Baseline_non_balanced                       0.784300                NaN             0.778200         0.046035\n",
      "\n",
      "‚úÖ Meilleur mod√®le: LightGBM_Baseline_balanced_FINAL_MODEL\n",
      "‚úÖ Run ID: cb9d5dadd7cb473fa009ee66284660fa\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## 6. Comparaison Finale et S√©lection du Meilleur Mod√®le\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä COMPARAISON FINALE DES MOD√àLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# R√©cup√©rer tous les runs de l'experiment\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "# Trier par business_cost\n",
    "runs_sorted = runs.sort_values(\"metrics.cv_business_cost_mean\", ascending=True)\n",
    "\n",
    "# Afficher le top 10 des mod√®les\n",
    "print(\"\\nTop 10 Mod√®les (tri√©s par co√ªt m√©tier):\")\n",
    "columns_to_show = [\n",
    "    \"tags.mlflow.runName\",\n",
    "    \"metrics.cv_business_cost_mean\",  # Correct\n",
    "    \"metrics.train_auc\",  # Correct\n",
    "    \"metrics.cv_auc_mean\",  # Ajout du pr√©fixe 'metrics.' ici !\n",
    "    \"metrics.auc_gap\",  # Si tu l'as loggu√©, c'est tr√®s utile √† voir\n",
    "]\n",
    "\n",
    "# On ne garde que les colonnes qui existent r√©ellement pour √©viter un nouveau crash\n",
    "existing_cols = [c for c in columns_to_show if c in runs_sorted.columns]\n",
    "\n",
    "print(runs_sorted[existing_cols].head(10).to_string(index=False))\n",
    "# S√©lectionner le meilleur\n",
    "best_run_id = runs_sorted.iloc[0][\"run_id\"]\n",
    "best_run_name = runs_sorted.iloc[0][\"tags.mlflow.runName\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Meilleur mod√®le: {best_run_name}\")\n",
    "print(f\"‚úÖ Run ID: {best_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9xgzz4m73oj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìÇ CHARGEMENT DES DONN√âES DE TEST\n",
      "============================================================\n",
      "‚úÖ Donn√©es de test charg√©es: (48744, 420)\n",
      "‚úÖ IDs de test: 48744 clients\n",
      "‚úÖ Pr√™t pour les pr√©dictions!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## 7. Chargement des Donn√©es de Test\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìÇ CHARGEMENT DES DONN√âES DE TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Charger les donn√©es de test preprocessed\n",
    "test_full = pd.read_csv(DATA_PATH / \"test_preprocessed.csv\")\n",
    "\n",
    "# S√©parer les IDs et les features\n",
    "test_ids = test_full[\"SK_ID_CURR\"]\n",
    "X_test = test_full.drop(\"SK_ID_CURR\", axis=1)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es de test charg√©es: {X_test.shape}\")\n",
    "print(f\"‚úÖ IDs de test: {len(test_ids)} clients\")\n",
    "print(f\"‚úÖ Pr√™t pour les pr√©dictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5386439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du mod√®le depuis : models:/Credit_Scoring_Model_LGBM/latest\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742d7bdd184d41a59ddbe692014c6eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Alignement termin√© : 420 -> 419 colonnes\n",
      "\n",
      "üß™ Test du Mod√®le en Production :\n",
      "============================================================\n",
      " SK_ID_CURR  Probability  Prediction\n",
      "     100001     0.242050           0\n",
      "     100005     0.635779           1\n",
      "     100013     0.248509           0\n",
      "     100028     0.180654           0\n",
      "     100038     0.518640           1\n",
      "\n",
      "üì§ G√©n√©ration des pr√©dictions finales pour Kaggle...\n",
      "‚úÖ Fichier de soumission cr√©√© : ../data/submission.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## 8. Model Serving - Test du Mod√®le en Production\n",
    "\"\"\"\n",
    "# S'assurer que le nom correspond √† ce qui a √©t√© enregistr√© plus haut\n",
    "model_name = \"Credit_Scoring_Model_LGBM\"\n",
    "\n",
    "# Charger la derni√®re version (au lieu de \"Production\" qui est peut-√™tre vide)\n",
    "model_uri = f\"models:/{model_name}/latest\"\n",
    "print(f\"Chargement du mod√®le depuis : {model_uri}\")\n",
    "\n",
    "model_production = mlflow.sklearn.load_model(\n",
    "    model_uri\n",
    ")  # Utilise sklearn.load_model pour garder predict_proba\n",
    "\n",
    "# 1. Aligner X_test sur les features du mod√®le de fa√ßon robuste\n",
    "features_cols = model_production.feature_name_\n",
    "\n",
    "# reindex ajoute les colonnes manquantes (fill_value=0) et retire les colonnes en trop\n",
    "X_test_aligned = X_test.reindex(columns=features_cols, fill_value=0)\n",
    "\n",
    "print(\n",
    "    f\"‚úÖ Alignement termin√© : {X_test.shape[1]} -> {X_test_aligned.shape[1]} colonnes\"\n",
    ")\n",
    "\n",
    "# 2. Test sur l'√©chantillon\n",
    "print(f\"\\nüß™ Test du Mod√®le en Production :\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_sample = X_test_aligned.head(5)\n",
    "predictions_proba = model_production.predict_proba(X_sample)[:, 1]\n",
    "predictions = (predictions_proba >= optimal_thresh).astype(int)\n",
    "\n",
    "# 3. Affichage\n",
    "result_df = pd.DataFrame(\n",
    "    {\n",
    "        \"SK_ID_CURR\": test_ids.head(5).values,\n",
    "        \"Probability\": predictions_proba,\n",
    "        \"Prediction\": predictions,\n",
    "    }\n",
    ")\n",
    "print(result_df.to_string(index=False))\n",
    "\n",
    "# 4. G√©n√©ration du fichier final\n",
    "print(\"\\nüì§ G√©n√©ration des pr√©dictions finales pour Kaggle...\")\n",
    "final_proba = model_production.predict_proba(X_test_aligned)[:, 1]\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    {\n",
    "        \"SK_ID_CURR\": test_ids,\n",
    "        \"TARGET\": final_proba,  # On envoie la proba pour maximiser l'AUC sur Kaggle\n",
    "    }\n",
    ")\n",
    "\n",
    "submission.to_csv(\"../data/submission.csv\", index=False)\n",
    "print(\"‚úÖ Fichier de soumission cr√©√© : ../data/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db58996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 9. Documentation et Export\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder le seuil optimal et autres m√©tadonn√©es\n",
    "metadata = {\n",
    "    \"best_model_name\": best_run_name,\n",
    "    \"best_model_run_id\": best_run_id,\n",
    "    \"optimal_threshold\": optimal_thresh,\n",
    "    \"cost_fn\": COST_FN,\n",
    "    \"cost_fp\": 1,\n",
    "    \"business_cost_optimal\": optimal_cost,  # ‚úÖ CORRECTION : cost_optimal ‚Üí optimal_cost\n",
    "    \"n_features\": X_train.shape[1],\n",
    "    \"n_train_samples\": X_train.shape[0],\n",
    "    \"class_distribution\": y_train.value_counts().to_dict(),\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"../models/model_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(\"\\n‚úÖ M√©tadonn√©es sauvegard√©es: model_metadata.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ MODELING TERMIN√â !\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "R√âSUM√â:\n",
    "- Meilleur mod√®le: {best_run_name}\n",
    "- Co√ªt m√©tier: {optimal_cost:.4f}\n",
    "- Seuil optimal: {optimal_thresh:.3f}\n",
    "- Mod√®le en production: {model_name}\n",
    "- Fichier soumission: submission.csv\n",
    "\n",
    "PROCHAINES √âTAPES:\n",
    "1. Lancer l'UI MLFlow: mlflow ui\n",
    "2. Comparer les runs et visualisations\n",
    "3. Soumettre √† Kaggle\n",
    "4. Documenter les r√©sultats\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
